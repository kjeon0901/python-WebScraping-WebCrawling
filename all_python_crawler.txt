웹페이지를 볼 때 어떠한 패턴으로 이루어졌는지를 먼저 봐야 함. 
	1. 네이버 웹페이지에서 '오늘 읽을만한 글' 中 '공연전시' 태그를 찾으려면, 오른쪽으로 넘어가는 버튼을 눌러야 함. → 버튼 코드 html에서 찾기
	2. '공연전시'를 누르고 나오는 화면은 내가 긁어온 html로 나오는 게 아니라, js 기능적으로 사이트 자체에서 태그 누르면 추가되도록 되어 있기 때문에, 내가 긁어온 html로는 알 수 없음. → Selenium 이용 필요
	3. 내가 찾고 싶은 class가 다른 곳에서도 쓰일 수 있다. → parent로 넘어감. 상위 태그에서 유일한 class를 가진 태그를 찾으면, 걔부터 접근
	※ 또 다른 방법이 있을 수 있음. 노하우 쌓이면 그렇게 하기. 

■■■■■■■■■■■■■0617 수업中 새로 알게된것■■■■■■■■■■■■■

     <!-- 
     id:하나의 id는 하나만 가리킴. class:여러 개 소속될수 있음.
     id는 #id{ }, class는 .class{ } 
     
     p.p-target : p라는 태그에서 p-target속성은 이렇게 디자인 해주세요~ 
     li.li-target : li라는 태그에서 li-target속성은 이렇게 디자인 해주세요~ 
     list-style-type: none; : 리스트 표시하는 앞의 표기 없애주세요~
     -->
    <!-- 
    한 칸 띄우면 '자식' !!
    
    div#container p : div라는 태그 중 id가 container인 애들의 '자식' 중에서 p태그 
    div#container div#wrap1 h3 : div라는 태그 중 id가 container인 애들의 '자식' 중에서 div태그 중 id가 wrap1인 애들의 '자식' 중에서 h3태그. 
    -->

■■■■■■■■■■■■■0618 수업中 새로 알게된것■■■■■■■■■■■■■

import requests as rq

url = "https://github.com/kjeon0901"

test = rq.get(url) # rq가 url에 get메시지를 보냄. 
                   # chrome, explorer 등 웹 브라우저를 거치지 않고 url이라는 서버에 접근해서 url웹페이지에 대한 코드를 가져옴. 
'''
파이썬에서는 그냥 rq가 url에 get메시지를 보낸 것 하나지만, 
서버에서는 해당 url에 대한 코드 html, css, javascript 등등을 보내주고, 그걸 실행함으로써 알아서 웹페이지가 열렸다. 

이제는 받아온 데이터 test에서 원하는 정보를 추출할 것임 !!
'''

res = rq.get(url)
print(res)
print(res.status_code)
'''
<Response [200]>
200


내가 어떤 요청을 하고 그게 수행되면 연산 결과 or 상태메시지에 대한 '응답 코드'가 온다. 
<Response [200]>    - 정상적으로 작동했다. 
                    - ex_ 로그인할 때 id, password를 입력하고 로그인 버튼 누르면, id, password를 담은 메시지가 request로 가고, 회원정보에 해당되면 200 반환
<Response [201]>    - 정상적으로 저장되었다. 
<Response [401]>    - 권한이 없다. 
... 수많은 응답 코드가 있으므로 외우지 말고 그때그때 찾아보자!

=> 웹크롤링 하는 도중 문제 해결이 필요하다면 웹에 대한 이런 기본 지식이 필요하다. 
'''

print(res.encoding) # utf-8형식으로 인코딩돼있구나~ 그럼 똑같이 utf-8로 디코딩해야겠구나~
'''utf-8'''

print(res.text) # 리턴받은 html 코드를 console창에 띄워줌. 
'''
......
 1.75h8.5A1.75 1.75 0 0014 13.25v-9.5a1.75 1.75 0 00-.874-1.515.75.75 0 10-.752 1.298.25.25 0 01.126.217v9.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-9.5a.25.25 0 01.126-.217z"></path>
</svg>
      <svg aria-hidden="true" viewBox="0 0 16 16" version="1.1" data-view-component="true" height="16" width="16" class="octicon octicon-check js-clipboard-check-icon color-text-success d-none m-2">
    <path fill-rule="evenodd" d="M13.78 4.22a.75.75 0 010 1.06l-7.25 7.25a.75.75 0 01-1.06 0L2.22 9.28a.75.75 0 011.06-1.06L6 10.94l6.72-6.72a.75.75 0 011.06 0z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>


  </body>
</html>
'''

cookies = list(res.cookies)
headers_cookies = res.headers['Set-Cookie']

print('cookies 속성')
print(cookies)
print('')
print('headers 속성')
print(headers_cookies)



res1 = rq.get(url, params={"key1": "value1", "key2": "value2"}) # query string 

print(res1.url)
'''
https://github.com/kjeon0901/?key1=value1&key2=value2

일반적으로 get 명령어에서는 파라미터 설정값을 url주소에다가 한꺼번에 붙여서 넘겨주는 방식 多 사용. 
- 딕셔너리 형태. 

어떤 설정값을 주느냐에 따라 웹페이지 정보가 바뀜. 

네이버 로그인
https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com
mode    form
url     https%3A%2F%2Fwww.naver.com

네이버 일회용 번호 로그인
https://nid.naver.com/nidlogin.login?mode=number&url=https%3A%2F%2Fwww.naver.com&locale=ko_KR&svctype=1
mode    number
url     https%3A%2F%2Fwww.naver.com
locale  ko_KR
svctype 1

'''

=============================================================

import requests as rq

url1 = "https://aldkfja.com/a" # https://aldkfja.com 라는 서버에서 a라는 웹페이지
res1 = rq.get(url1)
print(res1)
print(res1.status_code)
'''error'''

url2 = "https://github.com/kjeon0901/a" # https://github.com 라는 서버에서 kjeon0901 웹페이지 안의 a라는 웹페이지
res2 = rq.get(url2)
print(res2)
print(res2.status_code)
'''
<Response [404]>
404

서버 : https://github.com/
페이지 : https://github.com/kjeon0901 , https://github.com/kjeon0901/python-spider-machinelearning

url1. 서버 주소 자체를 잘못 입력한 경우 => '사이트에 연결할 수 없음', 코드 상으로는 아예 에러 남. 
url2. 서버에 들어갔는데 페이지가 없는 경우 => 404 뜸. 

프로그램이 돌다가 찾는 페이지 없어서 넘어가면 괜찮지만, 아예 에러가 나서 멈춰 있으면 안되니까 try-catch문 多 써줌! 
'''

def url_check(url):
    res = rq.get(url)

    print(res)

    sc = res.status_code

    if sc == 200:
        print("%s 요청성공"%(url))
    elif sc == 404:
        print("%s 찾을 수 없음" %(url))
    else:
        print("%s 알수 없는 에러 : %s"%(url, sc))


url_check("https://github.com/kjeon0901/")
url_check("https://github.com/kjeon0901//a")
'''
<Response [200]>
https://github.com/kjeon0901/ 요청성공
<Response [404]>
https://github.com/kjeon0901//a 찾을 수 없음
'''

=============================================================

import requests as rq

url = "https://blog.naver.com/kjeon0901"

res = rq.get(url)

print(res)
print(res.headers) # 딕셔너리 타입 
print(len(res.headers))
'''
<Response [200]>
{'Date': 'Fri, 18 Jun 2021 01:06:04 GMT', 'Content-Type': 'text/html;charset=UTF-8', 'Transfer-Encoding': 'chunked', 
 'Connection': 'close', 'Vary': 'Accept-Encoding', 'Cache-Control': 'no-cache', 'Expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 
 'Set-Cookie': 'JSESSIONID=7A8C9A4666E68B5D11A123D946BF986B.jvm1; Path=/; Secure; HttpOnly', 'Content-Encoding': 'gzip', 
 'Server': 'nxfps', 'Referrer-policy': 'unsafe-url'}
11
'''

headers = res.headers
print(headers['Set-Cookie']) # 딕셔너리 타입에 접근할 때에는 key값 이용 !
'''
Cookie
- ex) 내가 어떤 브라우저에서 로그인 한 뒤, 브라우저 껐다가 다시 켰는데 그대로 로그인 되어 있는 경우 '쿠키' 때문!
- '내가 이 브라우저에 접속했다' 는 게 작은 txt파일로 서버가 아니라 내 컴퓨터의 브라우저에 저장되는 것. 
  그러면 내가 브라우저에 다시 접속하면, 브라우저가 그 txt파일을 서버에 보내줘서 로그인 바로바로 되게 해줌. 
- 그런데 보안이 문제! 내 브라우저를 해킹하면 문제! => 조심스럽다. 
'''

for header in headers:
    print(headers[header])
'''
<Response [200]>
Fri, 18 Jun 2021 01:16:38 GMT
text/html;charset=UTF-8
chunked
close
Accept-Encoding
no-cache
Thu, 01 Jan 1970 00:00:00 GMT
JSESSIONID=C62E32687DD25DC10E313A7B5DD396E0.jvm1; Path=/; Secure; HttpOnly
gzip
nxfps
unsafe-url
'''

cookies = res.cookies
print(cookies)
'''
<RequestsCookieJar[<Cookie JSESSIONID=EB17C42065676C826ED5710E0E18AE21.jvm1 for blog.naver.com/>]>
내가 접속한 블로그에 대한 쿠키 파일이 남았다. 
'''

■■■■■■■■■■■■■0619 수업中 새로 알게된것■■■■■■■■■■■■■

ln [1] : 

from urllib.request import urlopen

html = urlopen('http://pythonscraping.com/pages/page1.html')
print(html.read())


ln [2] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup # 파싱(구문분석. 내가 원하는 정보만 취함)을 원활하게 해줌. 

html = urlopen('http://www.pythonscraping.com/pages/page1.html')
bs = BeautifulSoup(html.read(), 'html.parser') # html.read(), 즉 이 url의 html코드에서 파싱할 수 있게 해주는 객체. 
print(bs.h1) # 이 url의 코드에서 h1 태그만 출력. 
'''<h1>An Interesting Title</h1>'''


ln [3] : 

from urllib.request import urlopen
from urllib.error import HTTPError
from urllib.error import URLError

try:
    html = urlopen("https://pythonscrapingthisurldoesnotexist.com") # 정상적이지 않은 url이기 때문에 
except HTTPError as e: # 서버까지는 똑바로 접근했지만, 페이지가 없는 경우
    print("The server returned an HTTP error")
except URLError as e: # 서버 자체가 잘못된 경우 (현재 이 상황!)
    print("The server could not be found!")
else: # 위의 두 가지 에러 모두 발생하지 않은 경우
    print(html.read())
'''The server could not be found!'''


ln [4] : 

from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup


def getTitle(url): # 해당 url에서 title을 가져오는 함수
    try: 
        html = urlopen(url) # url 열어서 해당 페이지 html 가져옴
    except HTTPError as e: # 서버까지는 똑바로 접근했지만, 페이지가 없는 HTTPError 발생하면!
        print(e)
        return None
    try:
        bsObj = BeautifulSoup(html.read(), "lxml") # 이 html 소스 코드를 파싱하기 쉽게 하는 BeautifulSoup 객체 만듦.
        '''
        BeautifulSoup(html,'html.parser') 로 해도 됨. 
        lxml 모듈은 선택사항이다. bs4 모듈에 html parser는 내장되어있지만, lxml 의 속도가 더 빠르다. 
        '''
        title = bsObj.body.h1 # BeautifulSoup 객체가 html에서 body 태그 안의 h1 태그를 받아옴. 
        '''
        title1 = bsObj.body.h2 # 에러X. h2태그는 없음. 그냥 title1에는 None 들어가고 잘 실행됨. 
        title2 = title1.h1     # 에러O. title1=None이 들어가 있으므로 AttributeError 발생!
                                        => 이 코드는 수행 안 되고 except문으로 빠짐. 
                                        'NoneType' object has no attribute 'h1'
                                        Title could not be found
                               # 보통 이런 식으로 태그 안의 또 다른 태그에 접근하기 때문에 AttributeError 엄청 多多多!!
        '''
    except AttributeError as e: # 속성 오류 : 속성 이름 잘못됐거나, 없는 속성 가져오려 할 때 발생. 
        print(e)
        return None
    return title # except문(None 리턴하고 함수 종료되었음)으로 한 번도 빠지지 않은 경우, 여기서 title 리턴하게 됨. 


title = getTitle("http://www.pythonscraping.com/pages/page1.html") 
if title == None:
    print("Title could not be found")
else:
    print(title) # 이 코드에서는 다 잘 작동해서 제대로 print. 
    '''<h1>An Interesting Title</h1>'''

=============================================================

ln [] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
bs = BeautifulSoup(html, 'html.parser')
print(bs)


ln [1] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
bs = BeautifulSoup(html, "html.parser")


ln [2] : 

nameList = bs.findAll('span', {'class': 'green'}) # span 태그 中 class="green"(사람 이름 전부 이 속성 지정돼있음) 전부 찾아서
for name in nameList:
    print(name.get_text()) # 각각 text만 출력
'''
Anna
Pavlovna Scherer
Empress Marya
Fedorovna
Prince Vasili Kuragin
Anna Pavlovna
St. Petersburg
the prince
Anna Pavlovna
Anna Pavlovna
the prince
the prince
the prince
Prince Vasili
Anna Pavlovna
Anna Pavlovna
the prince
Wintzingerode
King of Prussia
le Vicomte de Mortemart
Montmorencys
Rohans
Abbe Morio
the Emperor
the prince
Prince Vasili
Dowager Empress Marya Fedorovna
the baron
Anna Pavlovna
the Empress
the Empress
Anna Pavlovna's
Her Majesty
Baron
Funke
The prince
Anna
Pavlovna
the Empress
The prince
Anatole
the prince
The prince
Anna
Pavlovna
Anna Pavlovna
'''


ln [3] : 

titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6']) # 리스트에 담긴 태그들 전부 찾아서 담음. 
print([title for title in titles]) # 찾아진 태그들 리스트에 담아서 출력


ln [4] : 

allText = bs.find_all('span', {'class':{'green', 'red'}}) # class="green" 또는 class="red"를 속성으로 갖는 모든 span 태그 찾아서 담음. 
print([text for text in allText]) # 찾아진 태그들 리스트에 담아서 출력


ln [5] : 

nameList = bs.find_all(text='the prince') # 이번에는 태그 속성이 아니라, 실질적으로 'the prince'라는 text에 집중해서 
                                          # 이 구문이 나올 때마다 리스트에 'the prince' 담아줌. 
print(nameList)
print(len(nameList))


ln [6] : 

title = bs.find_all(id='title', class_='text') # 이런 태그는 없어서 아무것도 안 담김. 
print([text for text in title])
'''
class_ : class 속성 검색하려면 class 말고 class_로 검색해야 함. 
'''


ln [7] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')

for child in bs.find('table',{'id':'giftList'}).children: # find : 조건을 만족시키는 가장 첫 번째 아이만 찾아서 리턴. 
                                                          # 참고로, <tbody></tbody>는 태그가 아님
    print(child)
    
'''
sibling : 형제
children : 자식(2대까지만)
descendant : 자손(2대, 3대, 4대, ...)
------------------------------------------


<tr><th>
Item Title
</th><th>
Description
</th><th>
Cost
</th><th>
Image
</th></tr>


<tr class="gift" id="gift1"><td>
Vegetable Basket
</td><td>
This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
<span class="excitingNote">Now with super-colorful bell peppers!</span>
</td><td>
$15.00
</td><td>
<img src="../img/gifts/img1.jpg"/>
</td></tr>


<tr class="gift" id="gift2"><td>
Russian Nesting Dolls
</td><td>
Hand-painted by trained monkeys, these exquisite dolls are priceless! And by "priceless," we mean "extremely expensive"! <span class="excitingNote">8 entire dolls per set! Octuple the presents!</span>
</td><td>
$10,000.52
</td><td>
<img src="../img/gifts/img2.jpg"/>
</td></tr>


<tr class="gift" id="gift3"><td>
Fish Painting
</td><td>
If something seems fishy about this painting, it's because it's a fish! <span class="excitingNote">Also hand-painted by trained monkeys!</span>
</td><td>
$10,005.00
</td><td>
<img src="../img/gifts/img3.jpg"/>
</td></tr>


<tr class="gift" id="gift4"><td>
Dead Parrot
</td><td>
This is an ex-parrot! <span class="excitingNote">Or maybe he's only resting?</span>
</td><td>
$0.50
</td><td>
<img src="../img/gifts/img4.jpg"/>
</td></tr>


<tr class="gift" id="gift5"><td>
Mystery Box
</td><td>
If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class="excitingNote">Keep your friends guessing!</span>
</td><td>
$1.50
</td><td>
<img src="../img/gifts/img6.jpg"/>
</td></tr>

'''


ln [8] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')

for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings: # find니까 맨 첫 번째 tr. 그 아이 다음에 있는 형제자매들 전부 담음. 
    print(sibling) 
'''
<tr class="gift" id="gift1"><td>
Vegetable Basket
</td><td>
This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
<span class="excitingNote">Now with super-colorful bell peppers!</span>
</td><td>
$15.00
</td><td>
<img src="../img/gifts/img1.jpg"/>
</td></tr>


<tr class="gift" id="gift2"><td>
Russian Nesting Dolls
</td><td>
Hand-painted by trained monkeys, these exquisite dolls are priceless! And by "priceless," we mean "extremely expensive"! <span class="excitingNote">8 entire dolls per set! Octuple the presents!</span>
</td><td>
$10,000.52
</td><td>
<img src="../img/gifts/img2.jpg"/>
</td></tr>


<tr class="gift" id="gift3"><td>
Fish Painting
</td><td>
If something seems fishy about this painting, it's because it's a fish! <span class="excitingNote">Also hand-painted by trained monkeys!</span>
</td><td>
$10,005.00
</td><td>
<img src="../img/gifts/img3.jpg"/>
</td></tr>


<tr class="gift" id="gift4"><td>
Dead Parrot
</td><td>
This is an ex-parrot! <span class="excitingNote">Or maybe he's only resting?</span>
</td><td>
$0.50
</td><td>
<img src="../img/gifts/img4.jpg"/>
</td></tr>


<tr class="gift" id="gift5"><td>
Mystery Box
</td><td>
If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class="excitingNote">Keep your friends guessing!</span>
</td><td>
$1.50
</td><td>
<img src="../img/gifts/img6.jpg"/>
</td></tr>

'''


ln [9] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')
print(bs.find('img',
              {'src':'../img/gifts/img1.jpg'}) # 이 src 경로를 속성으로 갖는 img태그를 찾아서
      .parent.previous_sibling.get_text()) # 그 태그의 부모태그 td를 찾고, td의 바로 앞 sibling인 또다른 td를 찾고, 걔의 text만 뽑아 print. 
'''$15.00'''


ln [10] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')
images = bs.find_all('img', {'src':re.compile('\.\.\/img\/gifts/img.*\.jpg')}) # re.compile(): 정규식 문자열. 
'''
지금 img 파일 src를 전부 보면 규칙성을 찾을 수 있다. img 뒤의 숫자값이 증가하면서 다음 img로 넘어간다. 
        => 정규식을 만족하는 아이들 전부 긁어오는 식으로 하면 되겠군!
'\.\.\/img\/gifts/img.*\.jpg'
        => . → 문자(글자,숫자,기호,공백 등) 하나 나타남 
        => * → 바로 앞의 문자 or 표현식이 0번 이상 나타남 (없을수도 o)
'''
for image in images: 
    print(image['src'])
'''
../img/gifts/img1.jpg
../img/gifts/img2.jpg
../img/gifts/img3.jpg
../img/gifts/img4.jpg
../img/gifts/img6.jpg
'''


ln [11] : 

bs.find_all(lambda tag: len(tag.attrs) == 2)


ln [12] : 

bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\'s only resting?')


ln [13] : 

bs.find_all('', text='Or maybe he\'s only resting?')



■■■■■■■■■■■■■0620 수업中 새로 알게된것■■■■■■■■■■■■■











■■■■■■■■■■■■■0621 수업中 새로 알게된것■■■■■■■■■■■■■









