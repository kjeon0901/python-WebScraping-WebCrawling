웹 스크래핑 Scraping	- 하나의 페이지에서만.
웹 크롤링 Crawling		- 페이지 안의 a태그(링크)를 통해 여러 웹페이지를 넘나듦.
웹페이지를 볼 때 어떠한 패턴으로 이루어졌는지를 먼저 봐야 함. 
	1. 네이버 웹페이지에서 '오늘 읽을만한 글' 中 '공연전시' 태그를 찾으려면, 오른쪽으로 넘어가는 버튼을 눌러야 함. → 버튼 코드 html에서 찾기
	2. '공연전시'를 누르고 나오는 화면은 내가 긁어온 html로 나오는 게 아니라, js 기능적으로 사이트 자체에서 태그 누르면 추가되도록 되어 있기 때문에, 내가 긁어온 html로는 알 수 없음. → Selenium 이용 필요
	3. 내가 찾고 싶은 class가 다른 곳에서도 쓰일 수 있다. → parent로 넘어감. 상위 태그에서 유일한 class를 가진 태그를 찾으면, 걔부터 접근
	※ 또 다른 방법이 있을 수 있음. 노하우 쌓이면 그렇게 하기. 
https://regexr.com/ 정규표현식과 문자열 match하는지 확인하는 사이트
F12에서 찾을때 => html코드 눌러서 Ctrl+f 누르면 찾는 곳 나옴. 
F12 - Network - 새로고침: 서버와 통신하면서 주고받은 애들 나옴
	Headers : 헤더정보 나옴
	Headers - Request Headers - user-agent : 내가 어떤 걸 이용해서 서버에 request 보냈는지에 대한 정보
세션 - 서버가 가지고 있음
쿠키 - 브라우저가 가지고 있음 (txt파일), html.headers에 있음. 
주소창에 입력하는 게 url = 서버주소 + 페이지주소
도메인주소 = 서버주소 = 어떤 ip주소(숫자값)를 나타냄
절대경로	> 문서의 위치를 가르키는 도메인을 포함한 전체 위치정보
상대경로	> 문서를 기준으로 한 다른 리소스들의 위치정보
	> 쓰는법 : 현재 파일의 전체경로와 도달 할 파일의 전체경로를 비교해서 중복되는 상위 폴더는 모두 지우고 ' ../ ' 를 넣어주자!

■■■■■■■■■■■■■0617 수업中 새로 알게된것■■■■■■■■■■■■■

     <!-- 
     id:하나의 id는 하나만 가리킴. class:여러 개 소속될수 있음.
     id는 #id{ }, class는 .class{ } 
     
     p.p-target : p라는 태그에서 p-target속성은 이렇게 디자인 해주세요~ 
     li.li-target : li라는 태그에서 li-target속성은 이렇게 디자인 해주세요~ 
     list-style-type: none; : 리스트 표시하는 앞의 표기 없애주세요~
     -->
    <!-- 
    한 칸 띄우면 '자식' !!
    
    div#container p : div라는 태그 중 id가 container인 애들의 '자식' 중에서 p태그 
    div#container div#wrap1 h3 : div라는 태그 중 id가 container인 애들의 '자식' 중에서 div태그 중 id가 wrap1인 애들의 '자식' 중에서 h3태그. 
    -->

■■■■■■■■■■■■■0618 수업中 새로 알게된것■■■■■■■■■■■■■

import requests as rq

url = "https://github.com/kjeon0901"

test = rq.get(url) # rq가 url에 get메시지를 보냄. 
                   # chrome, explorer 등 웹 브라우저를 거치지 않고 url이라는 서버에 접근해서 url웹페이지에 대한 코드를 가져옴. 
'''
파이썬에서는 그냥 rq가 url에 get메시지를 보낸 것 하나지만, 
서버에서는 해당 url에 대한 코드 html, css, javascript 등등을 보내주고, 그걸 실행함으로써 알아서 웹페이지가 열렸다. 

이제는 받아온 데이터 test에서 원하는 정보를 추출할 것임 !!
'''

res = rq.get(url)
print(res)
print(res.status_code)
'''
<Response [200]>
200


내가 어떤 요청을 하고 그게 수행되면 연산 결과 or 상태메시지에 대한 '응답 코드'가 온다. 
<Response [200]>    - 정상적으로 작동했다. 
                    - ex_ 로그인할 때 id, password를 입력하고 로그인 버튼 누르면, id, password를 담은 메시지가 request로 가고, 회원정보에 해당되면 200 반환
<Response [201]>    - 정상적으로 저장되었다. 
<Response [401]>    - 권한이 없다. 
... 수많은 응답 코드가 있으므로 외우지 말고 그때그때 찾아보자!

=> 웹크롤링 하는 도중 문제 해결이 필요하다면 웹에 대한 이런 기본 지식이 필요하다. 
'''

print(res.encoding) # utf-8형식으로 인코딩돼있구나~ 그럼 똑같이 utf-8로 디코딩해야겠구나~
'''utf-8'''

print(res.text) # 리턴받은 html 코드를 console창에 띄워줌. 
'''
......
 1.75h8.5A1.75 1.75 0 0014 13.25v-9.5a1.75 1.75 0 00-.874-1.515.75.75 0 10-.752 1.298.25.25 0 01.126.217v9.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-9.5a.25.25 0 01.126-.217z"></path>
</svg>
      <svg aria-hidden="true" viewBox="0 0 16 16" version="1.1" data-view-component="true" height="16" width="16" class="octicon octicon-check js-clipboard-check-icon color-text-success d-none m-2">
    <path fill-rule="evenodd" d="M13.78 4.22a.75.75 0 010 1.06l-7.25 7.25a.75.75 0 01-1.06 0L2.22 9.28a.75.75 0 011.06-1.06L6 10.94l6.72-6.72a.75.75 0 011.06 0z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>


  </body>
</html>
'''

cookies = list(res.cookies)
headers_cookies = res.headers['Set-Cookie']

print('cookies 속성')
print(cookies)
print('')
print('headers 속성')
print(headers_cookies)



res1 = rq.get(url, params={"key1": "value1", "key2": "value2"}) # query string 

print(res1.url)
'''
https://github.com/kjeon0901/?key1=value1&key2=value2

일반적으로 get 명령어에서는 파라미터 설정값을 url주소에다가 한꺼번에 붙여서 넘겨주는 방식 多 사용. 
- 딕셔너리 형태. 

어떤 설정값을 주느냐에 따라 웹페이지 정보가 바뀜. 

네이버 로그인
https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com
mode    form
url     https%3A%2F%2Fwww.naver.com

네이버 일회용 번호 로그인
https://nid.naver.com/nidlogin.login?mode=number&url=https%3A%2F%2Fwww.naver.com&locale=ko_KR&svctype=1
mode    number
url     https%3A%2F%2Fwww.naver.com
locale  ko_KR
svctype 1

'''

=============================================================

import requests as rq

url1 = "https://aldkfja.com/a" # https://aldkfja.com 라는 서버에서 a라는 웹페이지
res1 = rq.get(url1)
print(res1)
print(res1.status_code)
'''error'''

url2 = "https://github.com/kjeon0901/a" # https://github.com 라는 서버에서 kjeon0901 웹페이지 안의 a라는 웹페이지
res2 = rq.get(url2)
print(res2)
print(res2.status_code)
'''
<Response [404]>
404

서버 : https://github.com/
페이지 : https://github.com/kjeon0901 , https://github.com/kjeon0901/python-spider-machinelearning

url1. 서버 주소 자체를 잘못 입력한 경우 => '사이트에 연결할 수 없음', 코드 상으로는 아예 에러 남. 
url2. 서버에 들어갔는데 페이지가 없는 경우 => 404 뜸. 

프로그램이 돌다가 찾는 페이지 없어서 넘어가면 괜찮지만, 아예 에러가 나서 멈춰 있으면 안되니까 try-catch문 多 써줌! 
'''

def url_check(url):
    res = rq.get(url)

    print(res)

    sc = res.status_code

    if sc == 200:
        print("%s 요청성공"%(url))
    elif sc == 404:
        print("%s 찾을 수 없음" %(url))
    else:
        print("%s 알수 없는 에러 : %s"%(url, sc))


url_check("https://github.com/kjeon0901/")
url_check("https://github.com/kjeon0901//a")
'''
<Response [200]>
https://github.com/kjeon0901/ 요청성공
<Response [404]>
https://github.com/kjeon0901//a 찾을 수 없음
'''

=============================================================

import requests as rq

url = "https://blog.naver.com/kjeon0901"

res = rq.get(url)

print(res)
print(res.headers) # 딕셔너리 타입 
print(len(res.headers))
'''
<Response [200]>
{'Date': 'Fri, 18 Jun 2021 01:06:04 GMT', 'Content-Type': 'text/html;charset=UTF-8', 'Transfer-Encoding': 'chunked', 
 'Connection': 'close', 'Vary': 'Accept-Encoding', 'Cache-Control': 'no-cache', 'Expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 
 'Set-Cookie': 'JSESSIONID=7A8C9A4666E68B5D11A123D946BF986B.jvm1; Path=/; Secure; HttpOnly', 'Content-Encoding': 'gzip', 
 'Server': 'nxfps', 'Referrer-policy': 'unsafe-url'}
11
'''

headers = res.headers
print(headers['Set-Cookie']) # 딕셔너리 타입에 접근할 때에는 key값 이용 !
'''
Cookie
- ex) 내가 어떤 브라우저에서 로그인 한 뒤, 브라우저 껐다가 다시 켰는데 그대로 로그인 되어 있는 경우 '쿠키' 때문!
- '내가 이 브라우저에 접속했다' 는 게 작은 txt파일로 서버가 아니라 내 컴퓨터의 브라우저에 저장되는 것. 
  그러면 내가 브라우저에 다시 접속하면, 브라우저가 그 txt파일을 서버에 보내줘서 로그인 바로바로 되게 해줌. 
- 그런데 보안이 문제! 내 브라우저를 해킹하면 문제! => 조심스럽다. 
'''

for header in headers:
    print(headers[header])
'''
<Response [200]>
Fri, 18 Jun 2021 01:16:38 GMT
text/html;charset=UTF-8
chunked
close
Accept-Encoding
no-cache
Thu, 01 Jan 1970 00:00:00 GMT
JSESSIONID=C62E32687DD25DC10E313A7B5DD396E0.jvm1; Path=/; Secure; HttpOnly
gzip
nxfps
unsafe-url
'''

cookies = res.cookies
print(cookies)
'''
<RequestsCookieJar[<Cookie JSESSIONID=EB17C42065676C826ED5710E0E18AE21.jvm1 for blog.naver.com/>]>
내가 접속한 블로그에 대한 쿠키 파일이 남았다. 
'''

■■■■■■■■■■■■■0619 수업中 새로 알게된것■■■■■■■■■■■■■

### * Chapter01_BeginningToScrape

ln [1] : 

from urllib.request import urlopen

html = urlopen('http://pythonscraping.com/pages/page1.html')
print(html.read())


ln [2] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup # 파싱(구문분석. 내가 원하는 정보만 취함)을 원활하게 해줌. 

html = urlopen('http://www.pythonscraping.com/pages/page1.html')
bs = BeautifulSoup(html.read(), 'html.parser') # html.read(), 즉 이 url의 html코드에서 파싱할 수 있게 해주는 객체. 
print(bs.h1) # 이 url의 코드에서 h1 태그만 출력. 
'''<h1>An Interesting Title</h1>'''


ln [3] : 

from urllib.request import urlopen
from urllib.error import HTTPError
from urllib.error import URLError

try:
    html = urlopen("https://pythonscrapingthisurldoesnotexist.com") # 정상적이지 않은 url이기 때문에 
except HTTPError as e: # 서버까지는 똑바로 접근했지만, 페이지가 없는 경우
    print("The server returned an HTTP error")
except URLError as e: # 서버 자체가 잘못된 경우 (현재 이 상황!)
    print("The server could not be found!")
else: # 위의 두 가지 에러 모두 발생하지 않은 경우
    print(html.read())
'''The server could not be found!'''


ln [4] : 

from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup


def getTitle(url): # 해당 url에서 title을 가져오는 함수
    try: 
        html = urlopen(url) # url 열어서 해당 페이지 html 가져옴
    except HTTPError as e: # 서버까지는 똑바로 접근했지만, 페이지가 없는 HTTPError 발생하면!
        print(e)
        return None
    try:
        bsObj = BeautifulSoup(html.read(), "lxml") # 이 html 소스 코드를 파싱하기 쉽게 하는 BeautifulSoup 객체 만듦.
        '''
        BeautifulSoup(html,'html.parser') 로 해도 됨. 
        lxml 모듈은 선택사항이다. bs4 모듈에 html parser는 내장되어있지만, lxml 의 속도가 더 빠르다. 
        '''
        title = bsObj.body.h1 # BeautifulSoup 객체가 html에서 body 태그 안의 h1 태그를 받아옴. 
        '''
        title1 = bsObj.body.h2 # 에러X. h2태그는 없음. 그냥 title1에는 None 들어가고 잘 실행됨. 
        title2 = title1.h1     # 에러O. title1=None이 들어가 있으므로 AttributeError 발생!
                                        => 이 코드는 수행 안 되고 except문으로 빠짐. 
                                        'NoneType' object has no attribute 'h1'
                                        Title could not be found
                               # 보통 이런 식으로 태그 안의 또 다른 태그에 접근하기 때문에 AttributeError 엄청 多多多!!
        '''
    except AttributeError as e: # 속성 오류 : 속성 이름 잘못됐거나, 없는 속성 가져오려 할 때 발생. 
        print(e)
        return None
    return title # except문(None 리턴하고 함수 종료되었음)으로 한 번도 빠지지 않은 경우, 여기서 title 리턴하게 됨. 


title = getTitle("http://www.pythonscraping.com/pages/page1.html") 
if title == None:
    print("Title could not be found")
else:
    print(title) # 이 코드에서는 다 잘 작동해서 제대로 print. 
    '''<h1>An Interesting Title</h1>'''

=============================================================

### * Chapter02_AdvancedHTMLParsing

ln [] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
bs = BeautifulSoup(html, 'html.parser')
print(bs)


ln [1] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
bs = BeautifulSoup(html, "html.parser")


ln [2] : 

nameList = bs.findAll('span', {'class': 'green'}) # span 태그 中 class="green"(사람 이름 전부 이 속성 지정돼있음) 전부 찾아서
for name in nameList:
    print(name.get_text()) # 각각 text만 출력
'''
Anna
Pavlovna Scherer
Empress Marya
Fedorovna
Prince Vasili Kuragin
Anna Pavlovna
St. Petersburg
the prince
Anna Pavlovna
Anna Pavlovna
the prince
the prince
the prince
Prince Vasili
Anna Pavlovna
Anna Pavlovna
the prince
Wintzingerode
King of Prussia
le Vicomte de Mortemart
Montmorencys
Rohans
Abbe Morio
the Emperor
the prince
Prince Vasili
Dowager Empress Marya Fedorovna
the baron
Anna Pavlovna
the Empress
the Empress
Anna Pavlovna's
Her Majesty
Baron
Funke
The prince
Anna
Pavlovna
the Empress
The prince
Anatole
the prince
The prince
Anna
Pavlovna
Anna Pavlovna
'''


ln [3] : 

titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6']) # 리스트에 담긴 태그들 전부 찾아서 담음. 
print([title for title in titles]) # 찾아진 태그들 리스트에 담아서 출력


ln [4] : 

allText = bs.find_all('span', {'class':{'green', 'red'}}) # class="green" 또는 class="red"를 속성으로 갖는 모든 span 태그 찾아서 담음. 
print([text for text in allText]) # 찾아진 태그들 리스트에 담아서 출력


ln [5] : 

nameList = bs.find_all(text='the prince') # 이번에는 태그 속성이 아니라, 실질적으로 'the prince'라는 text에 집중해서 
                                          # 이 구문이 나올 때마다 리스트에 'the prince' 담아줌. 
print(nameList)
print(len(nameList))


ln [6] : 

title = bs.find_all(id='title', class_='text') # 이런 태그는 없어서 아무것도 안 담김. 
print([text for text in title])
'''
class_ : class 속성 검색하려면 class 말고 class_로 검색해야 함. 
'''


ln [7] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')

for child in bs.find('table',{'id':'giftList'}).children: # find : 조건을 만족시키는 가장 첫 번째 아이만 찾아서 리턴. 
                                                          # 참고로, <tbody></tbody>는 태그가 아님
    print(child)
    
'''
sibling : 형제
children : 자식(2대까지만)
descendant : 자손(2대, 3대, 4대, ...)
------------------------------------------


<tr><th>
Item Title
</th><th>
Description
</th><th>
Cost
</th><th>
Image
</th></tr>


<tr class="gift" id="gift1"><td>
Vegetable Basket
</td><td>
This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
<span class="excitingNote">Now with super-colorful bell peppers!</span>
</td><td>
$15.00
</td><td>
<img src="../img/gifts/img1.jpg"/>
</td></tr>


<tr class="gift" id="gift2"><td>
Russian Nesting Dolls
</td><td>
Hand-painted by trained monkeys, these exquisite dolls are priceless! And by "priceless," we mean "extremely expensive"! <span class="excitingNote">8 entire dolls per set! Octuple the presents!</span>
</td><td>
$10,000.52
</td><td>
<img src="../img/gifts/img2.jpg"/>
</td></tr>


<tr class="gift" id="gift3"><td>
Fish Painting
</td><td>
If something seems fishy about this painting, it's because it's a fish! <span class="excitingNote">Also hand-painted by trained monkeys!</span>
</td><td>
$10,005.00
</td><td>
<img src="../img/gifts/img3.jpg"/>
</td></tr>


<tr class="gift" id="gift4"><td>
Dead Parrot
</td><td>
This is an ex-parrot! <span class="excitingNote">Or maybe he's only resting?</span>
</td><td>
$0.50
</td><td>
<img src="../img/gifts/img4.jpg"/>
</td></tr>


<tr class="gift" id="gift5"><td>
Mystery Box
</td><td>
If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class="excitingNote">Keep your friends guessing!</span>
</td><td>
$1.50
</td><td>
<img src="../img/gifts/img6.jpg"/>
</td></tr>

'''


ln [8] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')

for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings: # find니까 맨 첫 번째 tr. 그 아이 다음에 있는 형제자매들 전부 담음. 
    print(sibling) 
'''
<tr class="gift" id="gift1"><td>
Vegetable Basket
</td><td>
This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
<span class="excitingNote">Now with super-colorful bell peppers!</span>
</td><td>
$15.00
</td><td>
<img src="../img/gifts/img1.jpg"/>
</td></tr>


<tr class="gift" id="gift2"><td>
Russian Nesting Dolls
</td><td>
Hand-painted by trained monkeys, these exquisite dolls are priceless! And by "priceless," we mean "extremely expensive"! <span class="excitingNote">8 entire dolls per set! Octuple the presents!</span>
</td><td>
$10,000.52
</td><td>
<img src="../img/gifts/img2.jpg"/>
</td></tr>


<tr class="gift" id="gift3"><td>
Fish Painting
</td><td>
If something seems fishy about this painting, it's because it's a fish! <span class="excitingNote">Also hand-painted by trained monkeys!</span>
</td><td>
$10,005.00
</td><td>
<img src="../img/gifts/img3.jpg"/>
</td></tr>


<tr class="gift" id="gift4"><td>
Dead Parrot
</td><td>
This is an ex-parrot! <span class="excitingNote">Or maybe he's only resting?</span>
</td><td>
$0.50
</td><td>
<img src="../img/gifts/img4.jpg"/>
</td></tr>


<tr class="gift" id="gift5"><td>
Mystery Box
</td><td>
If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class="excitingNote">Keep your friends guessing!</span>
</td><td>
$1.50
</td><td>
<img src="../img/gifts/img6.jpg"/>
</td></tr>

'''


ln [9] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')
print(bs.find('img',
              {'src':'../img/gifts/img1.jpg'}) # 이 src 경로를 속성으로 갖는 img태그를 찾아서
      .parent.previous_sibling.get_text()) # 그 태그의 부모태그 td를 찾고, td의 바로 앞 sibling인 또다른 td를 찾고, 걔의 text만 뽑아 print. 
'''$15.00'''


ln [10] : 

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')
images = bs.find_all('img', {'src':re.compile('\.\.\/img\/gifts/img.*\.jpg')}) # re.compile(): 정규표현식 
'''
지금 img 파일 src를 전부 보면 규칙성을 찾을 수 있다. img 뒤의 숫자값이 증가하면서 다음 img로 넘어간다. 
        => 정규식을 만족하는 아이들 전부 긁어오는 식으로 하면 되겠군!
'\.\.\/img\/gifts/img.*\.jpg'
        => \. → 문자열로서의 "."
        => \/ → 문자열로서의 "/"
        => .  → 기능으로서의  .  → 문자(글자,숫자,기호,공백 등) 하나 나타남 
        => *  → 기능으로서의  *  → 바로 앞의 문자 or 표현식이 0번 이상 나타남 (없을수도 o)
        => .* → 즉, 모든 문자열!!
'''
for image in images: 
    print(image['src'])
'''
../img/gifts/img1.jpg
../img/gifts/img2.jpg
../img/gifts/img3.jpg
../img/gifts/img4.jpg
../img/gifts/img6.jpg
'''


ln [11] : 

bs.find_all(lambda tag: len(tag.attrs) == 2)


ln [12] : 

bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\'s only resting?')


ln [13] : 

bs.find_all('', text='Or maybe he\'s only resting?')

■■■■■■■■■■■■■0621 수업中 새로 알게된것■■■■■■■■■■■■■

정규표현식
* 	바로 앞에 있는 문자, 표현식, 대괄호로 묶인 문자들이 0번 이상 나타남
+ 	바로 앞에 있는 문자, 표현식, 대괄호로 묶인 문자들이 1번 이상 나타남
[] 	대괄호 안에 있는 문자 中 하나가 나타남
()       ♣	그룹으로 묶인 표현식. 가장 먼저 봐야 함
{m, n} 	바로 앞에 있는 문자, 표현식, 대괄호로 묶인 문자들이 m번 이상, n번 이하 나타남
[^] 	대괄호 안의 ^ 바로 뒤에 있는 문자를 제외한 문자가 나타남
| 	|로 분리된 문자, 문자열, 표현식 중 하나가 나타남
. 	문자 하나(글자, 숫자, 기호, 공백 등)가 나타남
^ 	바로 뒤에 있는 문자, 표현식, 대괄호로 묶인 문자들이 문자열의 맨 앞에 나타남
\ 	특수 문자를 원래 의미대로 문자열처럼 쓰게 하는 escape 문자
$ 	바로 앞에 있는 문자, 표현식, 대괄호로 묶인 문자들이 문자열의 마지막이라는 뜻
	※ 안 써주면 사실상 .*(모든 문자열)이 마지막에 있는 것이나 마찬가지라 뒤에 뭐가 오든 전부 일치하게 됨
?! 	'포함하지 않는다'는 뜻. 문자열의 다른 부분에서도 특정 문자를 완벽히 배제하려면 ^과 $를 앞뒤에 써야 함

====================================================

### *Chapter03-web-crawlers.ipynb

ln [1] : 

## 케빈 베이컨의 6단계 법칙 : 인간관계는 6단계만 거치면 지구상 대부분의 사람과 연결될 수 있다는 사회 이론

from urllib.request import urlopen
from bs4 import BeautifulSoup 

html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')
bs = BeautifulSoup(html, 'html.parser')
for link in bs.find_all('a'):
    if 'href' in link.attrs: # link.attrs : link라는 태그(a 태그 中 하나)가 가진 속성들
        print(link.attrs['href']) # href 속성의 value값 출력 
'''
/wiki/Wikipedia:Protection_policy#semi
#mw-head
#searchInput
/wiki/Kevin_Bacon_(disambiguation)
/wiki/File:Kevin_Bacon_SDCC_2014.jpg
/wiki/Philadelphia,_Pennsylvania
/wiki/Kyra_Sedgwick
/wiki/Sosie_Bacon
#cite_note-1
/wiki/Edmund_Bacon_(architect)
/wiki/Michael_Bacon_(musician)
/wiki/Holly_Near
http://baconbros.com/
  ㆍㆍㆍ
엄청 많음!
'''


ln [2] : 

## Retrieving Articles Only

from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')
bs = BeautifulSoup(html, 'html.parser')
for link in bs.find('div', {'id':'bodyContent'}).find_all(  # link : 아래 find_all 만족시키는 a태그들 中 하나
    'a', href=re.compile('^(/wiki/)((?!:).)*$')):           # a태그의 속성에 대한 정규표현식
    '''
    '^(/wiki/)((?!:).)*$'
        => ^  → 바로 뒤의 문자 or 표현식으로 문자열 시작해야 함. ^( ) 괄호로 묶으면 문자열도 가능
        => $  → 바로 앞의 문자 or 표현식으로 문자열 끝내야 함. 안 써주면 뒤에 더 와도 되므로, 피하고 싶은 애들을 완벽히 제외하려면 써줌. 
        => ?! → 바로 뒤의 문자 or 표현식은 오면 안 됨
        => ?= → 바로 뒤의 문자 or 표현식은 와야 됨
        ▶ 문자열은 /wiki/로 시작하고, :가 포함되지 않는 아무 문자열 오고 나면 끝난다. 
        ▶ $ 안써주면 뒤에 :, 개행 등 더 와도 앞부분만 match되면 ㄱㅊ. :를 완벽히 제외하지 못하게 됨. 
    '''
    if 'href' in link.attrs:     # for문에서 걸러낸 link의 속성 中 href가 있다면
        print(link.attrs['href']) # 그 href 속성의 value값 출력
'''
/wiki/Kevin_Bacon_(disambiguation)
/wiki/San_Diego_Comic-Con
/wiki/Philadelphia
/wiki/Pennsylvania
/wiki/Kyra_Sedgwick
/wiki/Sosie_Bacon
/wiki/Edmund_Bacon_(architect)
/wiki/Michael_Bacon_(musician)
/wiki/Footloose_(1984_film)
/wiki/JFK_(film)
/wiki/A_Few_Good_Men
/wiki/Apollo_13_(film)
  ㆍㆍㆍ
엄청 많음! 그래도 이제 '^(/wiki/)((?!:).)*$' 에 해당되는 속성값만 출력됨. 
계속 다른 영화, 다른 배우에 관한 위키백과로만 타고 타고 .. 들어가기 위해서 이렇게 걸러준 것임. 
'''


ln [*] : 

## Random Walk

from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now()) # 시드값(랜덤수행방법)
def getLinks(articleUrl):
    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl)) 
                # http://en.wikipedia.org : 서버주소
                # {} 안에 articleUrl 넣어줌 => http://en.wikipedia.org/wiki/Kevin_Bacon : 페이지주소
    bs = BeautifulSoup(html, 'html.parser')
    return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))

links = getLinks('/wiki/Kevin_Bacon') # href=re.compile('^(/wiki/)((?!:).)*$') 만족하는 a태그들만 담김
while len(links) > 0: # 무한루프
    newArticle = links[random.randint(0, len(links)-1)].attrs['href'] # href값(링크) 전체를 랜덤으로 담음
    '''
	np.random.randint(m, n) : m ~ n-1 사이의 랜덤 숫자 1개 뽑기. m 안써주면 0부터~
	np.random.rand(m, n) : 0 ~ 1 의 '균일분포 표준정규분포' 랜덤 숫자를 (m, n) 크기로 생성.
        		균일분포 == 상수함수(연속) : 샘플링한 값의 range가 0 ~ 1이고 0.3 ~ 0,4 사이의 그래프와 x축간의 면적은 "0.3 ~ 0.4 사이의 값이 뽑힐 확률"
                                    		떨어진 간극만 같다면 걔네의 면적, 즉 확률도 같다. 
	np.random.randn(m, n) : 평균 0, 표준편차 1의 '가우시안 표준정규분포' 랜덤 숫자를 (m, n) 크기로 생성
        		정규분포 : 0.3 ~ 0.4 사이의 값이 뽑힐 확률과 0.4 ~ 0.5 사이의 값이 뽑힐 확률이 다르다. ex_히스토그램 distplot(정규분포, kde=True)
	np.random.normal(평균, 표준편차, size) : 주어진 평균, 표준편차의 정규분포를 size만큼 생성
    '''
    print(newArticle)
    links = getLinks(newArticle) # 현재 a태그 속성 href값(링크) 안의 또다른 a태그를 가져옴. 
'''
/wiki/Phoenix_Film_Festival
/wiki/Mad_Hot_Ballroom
/wiki/Fiorello_H._LaGuardia_High_School
/wiki/The_Beacon_School
/wiki/Innovation_Diploma_Plus_High_School
/wiki/Little_Red_School_House
/wiki/New_York_School_of_Applied_Design_for_Women
/wiki/Rice_High_School_(New_York)
/wiki/High_School_of_Fashion_Industries
/wiki/Hewitt_School
  ㆍㆍㆍ
이제는 실제로 링크 안의 링크로, 또 그 안의 링크로 .. 타고 타고 들어가는 Crawler 만들어짐. 
이런 코드 안에 어떤 동작을 하도록 기능을 넣어주면 Crawler가 동작하게 됨. 
'''


ln [4] : 

## Recursively crawling an entire site

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

pages = set()
'''
집합 자료형 set
1. 중복x   unique한값만 의미있음
2. 순서x

//가장많이쓰는 방법 : 하나의 row 또는 하나의 column을 리스트로 가져옴 → 집합자료형으로 캐스팅해서 하나씩 걸러냄 → 다시 리스트로 캐스팅
A=[1,4,2,3,1,2]
SA = set(A) 	//{1,4,2,3}
NEWA = list(SA) 	//[1,4,2,3]
'''
def getLinks(pageUrl):
    global pages
    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))
    bs = BeautifulSoup(html, 'html.parser')
    for link in bs.find_all('a', href=re.compile('^(/wiki/)')): # href 속성값이 '/wiki/'로 시작하는 모든 a태그 하나씩
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages: # 한 번 갔던 페이지는 중복적으로 들어가지 않도록
                # We have encountered a new page
                newPage = link.attrs['href']
                print(newPage)
                pages.add(newPage) # 한 번 들어온 페이지 주소는 pages에 추가
                getLinks(newPage) # 재귀함수 => Stack 메모리 문제로, 보통 함수가 끝나지 않고 1000번 정도 재귀함수 돌다 보면 명령어(함수)가 계속 쌓이기만 해서 죽어버림.
                                                # 컴파일한다는 것이 사실은 코드를 전부 어셈블리 명령어로 바꿔 순차적으로 수행하는 것이기 때문. 
                                  # 어떤 회사들은 실수 방지하려고 => 1. 재귀함수(stack영역) 쓰지 말아라! 2. 메모리에 동적할당(heap영역, 해제 필요) 하지 말아라!
getLinks('')
'''
계속 
/wiki/Wikipedia
/wiki/Wikipedia:Protection_policy#semi
/wiki/Wikipedia:Requests_for_page_protection
/wiki/Wikipedia:Requests_for_permissions
/wiki/Wikipedia:Protection_policy#extended
/wiki/Wikipedia:Lists_of_protected_pages
  ㆍㆍㆍ 받아오다가
most recent call last 뜸. 
'''


ln [5] : 

## Collecting Data Across an Entire Site

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))
    bs = BeautifulSoup(html, 'html.parser')
    try:
        print(bs.h1.get_text())
        print(bs.find(id ='mw-content-text').find_all('p')[0]) # 가장 상위에 있는 문단 찾음. 
        print(bs.find(id='ca-edit').find('span').find('a').attrs['href']) # 여기서 계속 except문으로 빠짐. 
        # 이렇게 find.find.find... 면 무.조.건. AttributeError 예외처리 필요!!
        # None을 받아오는 것까지는 Okay인데, None.find 가 되면 AttributeError!
        
    except AttributeError:
        print('This page is missing something! Continuing.')
    
    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href']
                print('-'*20)
                print(newPage)
                pages.add(newPage)
                getLinks(newPage) # 재귀함수
getLinks('') 
'''
Main Page
<p><i><b><a href="/wiki/The_Thankful_Poor" title="The Thankful Poor">The Thankful Poor</a></b></i> is an 1894 painting by the African-American painter <a href="/wiki/Henry_Ossawa_Tanner" title="Henry Ossawa Tanner">Henry Ossawa Tanner</a>. It is a <a href="/wiki/Genre_painting" title="Genre painting">genre painting</a> of two African Americans praying at a table and shares themes with Tanner's other works like <i><a href="/wiki/The_Banjo_Lesson" title="The Banjo Lesson">The Banjo Lesson</a></i> (1893). The painting is considered a milestone in <a href="/wiki/African-American_art" title="African-American art">African-American art</a>, notably for its countering of racial stereotypes by portraying <a href="/wiki/African-American_culture" title="African-American culture">African-American culture</a> in a dignified manner. This depiction was influenced by <a href="/wiki/Benjamin_Tucker_Tanner" title="Benjamin Tucker Tanner">Tanner's father</a> and the <a href="/wiki/African_Methodist_Episcopal_Church" title="African Methodist Episcopal Church">African Methodist Episcopal Church</a>. Despite its popularity with critics, <i>The Thankful Poor</i> was Tanner's last African-American genre work before he began to focus on biblical scenes. After remaining hidden for years, the painting was discovered in a storage closet of the <a href="/wiki/Pennsylvania_School_for_the_Deaf" title="Pennsylvania School for the Deaf">Pennsylvania School for the Deaf</a> in 1970, before being purchased by <a href="/wiki/Camille_Cosby" title="Camille Cosby">Camille</a> and <a href="/wiki/Bill_Cosby" title="Bill Cosby">Bill Cosby</a> in 1981 for their <a href="/wiki/Camille_O._and_William_H._Cosby_Collection_of_African_American_Art" title="Camille O. and William H. Cosby Collection of African American Art">private collection</a>. In 2020, the painting was sold by the Cosbys to Art Bridges, a foundation created by <a href="/wiki/Alice_Walton" title="Alice Walton">Alice Walton</a> for loaning artwork. (<b><a href="/wiki/The_Thankful_Poor" title="The Thankful Poor">Full article...</a></b>)
</p>
This page is missing something! Continuing.
--------------------
/wiki/Wikipedia
Wikipedia
<p class="mw-empty-elt">
</p>
This page is missing something! Continuing.
--------------------
/wiki/Wikipedia:Protection_policy#semi
Wikipedia:Protection policy
<p class="mw-empty-elt">
</p>
This page is missing something! Continuing.
--------------------
  ㆍㆍㆍ
'''

■■■■■■■■■■■■■0622 수업中 새로 알게된것■■■■■■■■■■■■■

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

######### 문제 1. 재귀횟수 10번 (처음 실행까지 11번) 되면 프로그램 종료가 되는 코드 추가하라. 
'''
    # 재귀함수가 for문 안에 있기 때문에, 계속 for문의 첫 번째 loop만 만난다. 
    # 그러다가 a태그가 없어서 for문으로 들어오지 않고 이 getLinks함수가 종료되면, 
    # 이 함수가 호출된 곳으로 돌아가서 첫 번째 loop 끝나고 두 번째 loop로 넘어가게 된다. 
    # ex_
    #   getLinks()
    #       for-1
    #           getLinks()
    #               for-1
    #                   getLinks()
    #               for-2
    #                   getLinks()
    #                       for-1
    #                           getLinks()
    #       for-2
    #   ...
'''



'''
# 풀이 1. 

import sys

pages = set()
recursion = 10
def getLinks(pageUrl):
    global pages
    global recursion
    if recursion==-1:
        sys.exit("종료")
    if recursion <10:
        print("=============== 재귀", 10-recursion, "===============")

    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))
    bs = BeautifulSoup(html, 'html.parser')
    try:
        print(bs.h1.get_text())
        print(bs.find(id ='mw-content-text').find_all('p')[0]) # 가장 상위에 있는 문단 찾음. 
        print(bs.find(id='ca-edit').find('span').find('a').attrs['href']) # 여기서 계속 except문으로 빠짐. 
        # 이렇게 find.find.find... 면 무.조.건. AttributeError 예외처리 필요!!
        # None을 받아오는 것까지는 Okay인데, None.find 가 되면 AttributeError!
        
    except AttributeError:
        print('This page is missing something! Continuing.')
    
    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href']
                print('-'*20)
                print(newPage)
                pages.add(newPage)
                recursion-=1
                getLinks(newPage) # 재귀함수

getLinks('') 
'''


# 풀이 2. --good

pages = set()
recursion = 10
def getLinks(pageUrl):
    global pages
    global recursion
    if recursion==-1:
        return
    if recursion <10:
        print("=============== 재귀", 10-recursion, "===============")

    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))
    bs = BeautifulSoup(html, 'html.parser')
    try:
        print(bs.h1.get_text())
        print(bs.find(id ='mw-content-text').find_all('p')[0])
        print(bs.find(id='ca-edit').find('span').find('a').attrs['href'])
        
    except AttributeError:
        print('This page is missing something! Continuing.')
    
    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href']
                print('-'*20)
                print(newPage)
                pages.add(newPage)
                recursion-=1
                return getLinks(newPage) # 재귀함수. 
                # 여기서 return 안해주면 재귀 11만 넘어가고 재귀 12부터 또 이어서 하게 됨. 
    # for문 안 들어갔을 때, 여기서 이번 getLinks()함수는 아무 return 없이 그냥 끝나버림
            
getLinks('')

=============================================================

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

######### 문제 2. 재귀횟수 9번 (처음 실행까지 10번) => 웹페이지 10개 크롤링할 것임. 
######### kevin bacon 위키백과 Contents 목차 "1순위들만"을 crawling해서 목차에 대한 데이터를 리스트로 담기
######### [페이지제목(Kevin Bacon), Contents 1., Contents 2., Contents 3.] 이거 10개를 또 큰 리스트에 담기

pages = set()
recursion = 9
total_contents = []
def getLinks(pageUrl):
    global pages
    global recursion
    global total_contents
    if recursion==-1:
        return
    if recursion <9:
        print("=============== 재귀", 9-recursion, "===============")

    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))
    bs = BeautifulSoup(html, 'html.parser')
    contents = []
    try: # 목차 리스트 담기
        contents.append(bs.h1.get_text())
        all_li = bs.find('div', {'id':"toc"}).ul.find_all('li', class_=re.compile('^(toclevel-1 tocsection-).*$'))
        # 현재 all_li에 들어있는 건 li 하나가 아닌 li 리스트 뭉텅이가 들어 있기 때문에 all_li.find('span', {'class':'toctext'}).get_text()) 이렇게 바로 접근하면 안 됨!!
        # => AttributeError나서 except문으로 빠지게 됨
        for li in all_li: # 그래서 이렇게 for문으로 넘겨줘서 여기서 find해준다!
            contents.append(li.find('span', {'class':'toctext'}).get_text()) # 대신 list.a.get_text() 해줘도 된다. 


    except AttributeError:
        print('This page is missing something! Continuing.')
    print(contents)
    total_contents.append(contents)
    
    for link in bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')):
        if 'href' in link.attrs:
            if '(disambiguation)' in link.attrs['href']:
                continue
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href']
                print('-'*20)
                print(newPage)
                pages.add(newPage)
                recursion-=1
                return getLinks(newPage) # 재귀함수. 
            
getLinks('/wiki/Kevin_Bacon')

■■■■■■■■■■■■■0623 수업中 새로 알게된것■■■■■■■■■■■■■

뷰티풀솝 : 파싱 - 기본으로 사용
	from bs4 import BeautifulSoup
	bs = BeautifulSoup(html.text)
셀레니움 : 클릭(웹페이지 넘어갈 때 多), 타이핑, 마우스 움직이기 등 - 필요할 때 사용
	from selenium import webdriver
	driver = webdriver.Chrome('~~~/chromedriver_win32/chromedriver.exe')
	driver.get(url)
두 개를 같이 쓰자. 

태그 찾기
	- driver.find_element_by_id( ) 	: id로 태그 찾기
	- driver.find_element_by_class( ) 	: class로 태그 찾기
	- driver.find_element_by_tag_name( )	: tag name으로 태그 찾기
	- driver.find_element_by_xpath( )	: Copy Full XPath로 태그 찾기
	  ㆍㆍㆍ

driver.find_element_by_...	→ bs.find( )과 유사, 하나의 태그만 가져옴
driver.find_elements_by_...	→ bs.find_all( )과 유사, 모든 태그 가져옴

태그 위치 불러오기 : 개발자도구 F12 → 원하는 부분의 html코드 우클릭 → Copy → Copy Full XPath
	ex_  /html/body/header/div/nav/label,  /html/body/main/div/div/div[2]/h3

driver.~~~.click() : 태그 클릭하기
driver.~~~.get('key') : 태그의 해당 value값 가져오기
driver.current_url : 현재 페이지 주소

■■■■■■■■■■■■■0624 수업中 새로 알게된것■■■■■■■■■■■■■

from selenium import webdriver
import time

#### Selenium으로 url 자동으로 열어 검색창에 검색해보기


driver = webdriver.Chrome('C:/Users/hs-702/Desktop/kjeon/chromedriver_win32/chromedriver.exe')
url1 = 'https://www.naver.com'
url2 = 'https://www.google.com'
url3 = 'https://github.com/kjeon0901'

driver.get(url1)
# 네이버 검색창 input id : query
n_search_box = driver.find_element_by_id("query")
n_search_box.send_keys("크루엘라 평점") # 검색할 문자열을 키로 보낸 후
n_search_box.submit() # 검색버튼 눌러줌
time.sleep(1)

driver.get(url2)
# 구글 검색창 input name : q
g_search_box = driver.find_element_by_name("q")
g_search_box.send_keys("빌보드 차트 순위")
g_search_box.submit()
time.sleep(1)

driver.get(url3)
time.sleep(1)
driver.quit()

=============================================================

from selenium import webdriver
from selenium.webdriver.common.keys import Keys 
from selenium.webdriver.chrome.options import Options
import time

#### Selenium으로 네이버 자동 로그인해보기


path = 'C:/Users/hs-702/Desktop/kjeon/chromedriver_win32/chromedriver.exe'
url = 'https://naver.com'

id_ = ""
pw_ = ""

# 크롬 옵션 정의 (1이 허용, 2가 차단)
chrome_options = Options() 
prefs = {"profile.default_content_setting_values.notifications": 2} 
chrome_options.add_experimental_option("prefs", prefs)

driver = webdriver.Chrome(path, options=chrome_options)
driver.get(url)

click = driver.find_element_by_class_name("link_login")
click.click()
login_box = driver.find_element_by_id("id")
login_box.send_keys(id_)
time.sleep(1)
login_box = driver.find_element_by_id("pw")
login_box.send_keys(pw_)
time.sleep(1)
login_box.send_keys(Keys.RETURN) # Keys.ENTER도 가능
time.sleep(1) 

driver.quit()

=============================================================

from bs4 import BeautifulSoup
from selenium import webdriver
import time

#### BeautifulSoup, Selenium 같이 사용하여, 네이버 사전 자동으로 열어 태그 text 긁어오기

# Selenium 객체 driver로 페이지 클릭해 접속하기
driver = webdriver.Chrome('C:/Users/hs-702/Desktop/kjeon/chromedriver_win32/chromedriver.exe')
url = 'https://www.naver.com'
driver.get(url)

# 네이버에서 사전 클릭하여 네이버 사전 페이지로 접속하기
driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/div[1]/div[1]/ul[2]/li[1]/a').click()

# BeurifulSoup 객체 bs로 html 파싱해서 원하는 text 가져오기
bs = BeautifulSoup(driver.page_source) # 여기서 html 넣어줘야 하므로 (html == driver.page_source)

temp = bs.find('div', {'id':'content'}).find_all('h2')
for var in temp:
    print(var.get_text())

time.sleep(1)
driver.quit()

■■■■■■■■■■■■■0625 수업中 새로 알게된것■■■■■■■■■■■■■

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from urllib.request import urlopen
import time

url = 'https://www.naver.com'
'''
url = 'https://news.naver.com' 로 바꾸면 => Remote end closed connection without response 에러 뜸. 
url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=102&oid=448&aid=0000332119' 이렇게 기사 하나 접근해도 에러 뜸. 
웹브라우저를 거치지 않고 파이썬으로 urlopen 이라는 request를 통해 직접 html을 가져오는 게 훨씬 빠름.
근데, 웹브라우저로 접근하면 접근 가능하고, 파이썬으로 접근하면 접근 에러 뜸. 
    => 네이버에서 브라우저로 접근하는 것 vs. 파이썬으로 접근하는 것 구분짓는 어떤 걸 가지고 있구나!
    => ★★★ 얘를 속여서 fake로 우회하는 방법이 필요하겠군!! ★★★
'''
html = urlopen(url)
print(html.headers)
'''
Server: NWS
Date: Fri, 25 Jun 2021 01:44:33 GMT
Content-Type: text/html; charset=UTF-8
Transfer-Encoding: chunked
Connection: close
Set-Cookie: PM_CK_loc=3232355beb513edf3f28c441f6ad912584637d53608e6f52db25aa25b48df3fb; Expires=Sat, 26 Jun 2021 01:44:33 GMT; Path=/; HttpOnly
Cache-Control: no-cache, no-store, must-revalidate
Pragma: no-cache
P3P: CP="CAO DSP CURa ADMa TAIa PSAa OUR LAW STP PHY ONL UNI PUR FIN COM NAV INT DEM STA PRE"
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Strict-Transport-Security: max-age=63072000; includeSubdomains
Referrer-Policy: unsafe-url
--------------------------------------------------------------------------------------
HTTP 쿠키
    - 서버가 브라우저에게 전송하는 데이터
    - 클라이언트는 이를 작은 txt파일로 저장한 다음에 다시 서버에게 전송
Set-Cookie
    - 서버는 Set-Cookie 헤더를 설정하여 응답과 함께 전송하고 쿠키는 브라우저에 저장
'''

===========================================================

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from urllib.request import urlopen
import time

url = 'https://www.naver.com'

html2 = requests.get(url)
bs = BeautifulSoup(html2.text)
print(html2.request.headers)
'''
{'User-Agent': 'python-requests/2.24.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
--------------------------------------------------------------------------------------
User-Agent
    - 내가 어떤 걸 이용해서 서버에 request 보냈는지에 대한 정보!
    - 여기서 'python-requests/2.24.0'가 들어있으므로 "내가 파이썬이다!"라는 걸 알려주면서 네이버에 접근하게 되는 것!
        => 이걸 살짝 바꿔서 우회해주자. 
'''

============================================================

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from urllib.request import urlopen
import time

# url = 'https://www.naver.com'
url = 'https://news.naver.com'

var = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}
# 네이버에서 F12 - Network - 새로고침 - Headers - Request Headers - user-agent 가져와서 이렇게 key, value값으로 넣어줌
html3 = requests.get(url, headers = var) # 그 정보를 headers로 넣어줌. 
bs = BeautifulSoup(html3.text)
print(html3.request.headers)
'''
{'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
--------------------------------------------------------------------------------------
User-Agent
    - 아까와 다르게 'python-requests/2.24.0'이 아니라 '~~~ Chrome/91.0.4472.114 Safari/537.36'이 들어 있으므로
      "내가 크롬이다!"라는 걸 알려주면서 네이버에 접근하게 되는 것!

=> 우회 잘 되었군!
url = 'https://www.naver.com'
url = 'https://news.naver.com'
둘 다 잘 접근 잘 된다. 
'''

=============================================================

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from urllib.request import urlopen
import time

#### BeautifulSoup, Selenium 같이 사용
#### [네이버 뉴스] - 7분야 각각의 탭에서 카테고리 + 5가지 요약문 + 기사별 url(상대경로도 모두 절대경로로) + 기사별 입력날짜데이터 출력, 리스트에 넣기
'''
각각의 기사입력 날짜데이터 출력   - RemoteDisconnected: Remote end closed connection without response 접근 에러
                                - 우회 필요
+ 너무 빨리 계속하면 네이버에서 내 ip를 아예 차단시켜 버림. time.sleep(0.5)로 딜레이하도록 !
'''

url = 'https://naver.com'
driver = webdriver.Chrome('C:/Users/hs-702/Desktop/kjeon/chromedriver_win32/chromedriver.exe')
driver.get(url)
driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[3]/div[1]/div[1]/ul[2]/li[2]/a').click()

bs = BeautifulSoup(driver.page_source)
var = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'} # 우회하기 위함

total = []
for idx, category in enumerate(bs.find_all('div', {'class':'main_component droppable'})):
    category_total = []
    print("\n======",category.h4.get_text(), "======")
    for idx2, title in enumerate(category.find('ul').find_all('li')):
        '''
        1. children 쓰면 보이지 않는 잡다한 애들까지 다 잡히는 경우 多   →   find_all('li') 처럼 태그를 딱 지칭해주는 게 BETTER
        2. 이 부분에선 사실 'ul'태그가 하나씩밖에 없음   →   여기선 굳이 {'class':{'hdline_article_list', 'mlist2 no_bg'}} 이렇게 특정하지 않아도 됨. 
        '''
        if idx==0:
            news_title = title.a.get_text()
            news_url = driver.current_url + title.a.get("href")
        else:
            news_title = title.a.strong.get_text()
            news_url = title.a.get("href")
        print(news_title) # 기사별 요약문 출력
        print(news_url) # 기사별 url(상대경로도 모두 절대경로로) 출력
        
        html = requests.get(news_url, headers = var) # var에 담긴 fake user-agent 정보를 headers로 넣어줌. 
        bs2 = BeautifulSoup(html.text)
        news_date = bs2.find('span', {'class':'t11'}).get_text()
        print(news_date) # 기사별 입력날짜데이터 출력
        time.sleep(0.5)
        
        category_total.append([news_title, news_url, news_date]) # 리스트에 담기
    total.append(category_total)
        
#driver.quit()

■■■■■■■■■■■■■0628 수업中 새로 알게된것■■■■■■■■■■■■■

## Crawling across the Internet

ln [6] : 










■■■■■■■■■■■■■0629 수업中 새로 알게된것■■■■■■■■■■■■■
■■■■■■■■■■■■■0630 수업中 새로 알게된것■■■■■■■■■■■■■